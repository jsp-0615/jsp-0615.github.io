<!DOCTYPE HTML>
<html class="no-js" lang="zh-Hans">
<head>
    <!--[if lte IE 9]>
    <meta http-equiv="refresh" content="0;url=https://jsp-0615.github.io/warn.html">
<![endif]-->

<meta charset="utf-8" />
<title>transformer | Jasper自学笔记</title>
<meta name="keywords" content="" />
<meta name="description" content="Transformer整体结构

注意力机制Attention
简单来说，注意力机制描述了（序列）元素的加权平均值，其权重是根据输入的query和元素的键值进行动态计算的，具体来讲，有4个概念要明确：

Query：Query是一个特征向量，描述我们在序列中寻找什么，即我们可能想要注意什么
Keys：每个输入元素有一个键，它也是一个特征向量，该特征向量粗略的描述了该元素“提供”什么，或者他何时可能" />
<meta name="author" content="Jasper" />
<meta name="applicable-device" content="pc,mobile" />
<meta name="format-detection" content="telephone=no,email=no" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-title" content="Jasper自学笔记" />
<meta name="renderer" content="webkit" />
<meta name="force-rendering" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover" />
<meta property="og:type" content="article" />
<meta property="og:title" content="transformer | Jasper自学笔记" />
<meta property="og:description" content="Transformer整体结构

注意力机制Attention
简单来说，注意力机制描述了（序列）元素的加权平均值，其权重是根据输入的query和元素的键值进行动态计算的，具体来讲，有4个概念要明确：

Query：Query是一个特征向量，描述我们在序列中寻找什么，即我们可能想要注意什么
Keys：每个输入元素有一个键，它也是一个特征向量，该特征向量粗略的描述了该元素“提供”什么，或者他何时可能" />
<meta property="og:url" content="https://jsp-0615.github.io" />
<meta property="og:site_name" content="Jasper自学笔记" />

<link rel="dns-prefetch" href="https://jsp-0615.github.io" />
<link rel="preconnect" href="https://jsp-0615.github.io" />

    <link rel="preconnect" href="https://.disqus.com" />
    <link rel="preconnect" href="https://c.disquscdn.com" />


<link rel="icon" type="image/svg+xml" href="https://jsp-0615.github.io/images/favicon.svg" />
<link rel="alternate icon" type="image/png" href="https://jsp-0615.github.io/images/favicon.png" />
<link rel="alternate" type="application/xml" title="Site Map" href="https://jsp-0615.github.io/sitemap.xml"/>
<link rel="canonical" href="https://jsp-0615.github.io/knowledgeNotes/2025/04/24/transformers/" />


<link rel="stylesheet" href="/css/fonts/fontawesome-webfont.woff2?v=4.7.0" as="font" type="font/woff2" media="print" onload="this.media='all'" crossorigin />
<link rel="stylesheet" href="/css/JSimple.min.css" />

<style id="highl_css"></style>
<!-- MathJax 支持 -->
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<script type="text/javascript">
    (function() {
        let jsi_config = {
            isPost: 'true',
            creationTime: '12/04/2025',
            highlightTheme: 'atom-one',
            readMode: 'day',
            chatLink: '',
            localSearch: { dbPath: '' }
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.xml',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        window.jsi_config = jsi_config;
    })()
</script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="JSM">JSM</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>首页</span></a>
        <a href="/archives" title="归档"><i class="fa fa-archives"></i><span>归档</span></a>
        <a href="/tags" title="标签"><i class="fa fa-tags"></i><span>标签</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/about" title="About">
            <i class="fa fa-question-circle"></i>
            <span>About</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#" aria-label="content searching link"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#" aria-label="reading mode chang"><i class="fa fa-sun-o"></i></a>
    
</div>
<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <img class="cover-img" src="/images/cover-day.webp" alt="cover-img" loading="lazy"/>
    <div class="cover-info">
        <img class="avatar" src="/images/avatar.webp" width="100" alt="avatar" loading="lazy"/>
        <h1>Jasper知识库</h1>
        <h3></h3>
        <h4>全栈DS/DA工程师养成记</h4>
        <div class="cover-sns">
            
    <div class="btn btn-github">
        <a href="https://github.com/jsp-0615" title="github" rel="external nofollow noopener" target="_blank">
            <i class="fa fa-github"></i>
        </a>
    </div>

    <div class="btn btn-youtube">
        <a href="" title="youtube" rel="external nofollow noopener" target="_blank">
            <i class="fa fa-youtube"></i>
        </a>
    </div>


        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">最新</a></li>
        
            
                <li class="">
                    <a href="/categories/knowledgeNotes" data-name="知识笔记">知识笔记</a>
                </li>
            
                <li class="">
                    <a href="/categories/outcomes" data-name="成果展示">成果展示</a>
                </li>
            
                <li class="">
                    <a href="/categories/problemNotes" data-name="问题笔记">问题笔记</a>
                </li>
            
                <li class="">
                    <a href="/categories/grocery" data-name="杂货铺">杂货铺</a>
                </li>
            
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text"
               readonly="readonly"
               id="local-search-input-tip"
               placeholder="按Shift启动搜索" />
        <button type="button" aria-label="fa-search" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope itemtype="https://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix" itemscope itemtype="https://schema.org/Person">
                <a class="avatar fleft" target="_blank" href="https://github.com/jsp-0615" rel="author noopener">
                    <img width="48" src="/images/avatar.webp" alt="avatar" itemprop="image"/>
                </a>
                <p><span class="label">作者</span>
                    <a target="_blank" rel="noopener" href="https://github.com/jsp-0615" itemprop="url"><span itemprop="name">Jasper</span></a>
                    <span itemprop="datePublished" content="2025-04-24T22:29:41.590+10:00">最后编辑于&nbsp;2025-04-24</span>
                </p>
                <p itemprop="description">一个学习记录网站</p>
            </div>
            <h1 class="post-title">
                <a target="_self" href="https://jsp-0615.github.io/knowledgeNotes/2025/04/24/transformers/" itemprop="mainEntityOfPage"><span itemprop="headline">Transformer</span></a>
            </h1>
            <div class="post-meta" itemprop="wordCount">
                本文共计8387个字 |
                <span id="busuanzi_container_page_pv">
                    你是第&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>位到访读者
                </span>
            </div>
        </div>
        <div class="post-content markdown-body articleBody" itemprop="articleBody">
            <h2 id="Transformer整体结构">Transformer整体结构</h2>
<p><img src="/images/transformers.png" alt="Transformer模型详解（图解最完整版）" /></p>
<h3 id="注意力机制Attention">注意力机制Attention</h3>
<p>简单来说，注意力机制描述了（序列）元素的加权平均值，其权重是根据输入的query和元素的键值进行动态计算的，具体来讲，有4个概念要明确：</p>
<ul>
<li>Query：Query是一个特征向量，描述我们在序列中寻找什么，即我们可能想要注意什么</li>
<li>Keys：每个输入元素有一个键，它也是一个特征向量，该特征向量粗略的描述了该元素“提供”什么，或者他何时可能很重要，键的涉及应该使我们可以根据Query来识别我们想要关注的元素</li>
<li>Values：每个输入元素，我们还有一个值向量，这个向量就是我们想要平均的向量</li>
<li>Score function：评分函数，为了对想要关注的元素进行评分，我们需要指定要给评分函数f该函数将查询和键作为输入，并输出查询-键对的得分/注意力权重，它通常通过简单的相似性度量来实现，例如点积或MLP</li>
</ul>
<p>$$<br />
\alpha_i = \frac{\exp\left( f_{\text{attn}}(\text{key}<em>i, \text{query}) \right)}<br />
{\sum_j \exp\left( f</em>{\text{attn}}(\text{key}_j, \text{query}) \right)},<br />
\quad<br />
\text{out} = \sum_i \alpha_i \cdot \text{value}_i<br />
$$</p>
<p>下图直观描述注意力如何作用在一系列单词上。对于每个单词，都有一个键和一个值向量。使用评分函数（在本例中为点积）将query与所有键进行比较以确定权重。最后，使用注意力权重对所有单词的值向量进行平均。（为了简单起见，softmax 没有可视化。）</p>
<p><img src="/images/how_qvk_cal.webp" alt="img" /></p>
<p>大多数注意力机制在使用哪些query、如何定义键、值向量，以及使用什么评分函数方面有所不同。</p>
<p>Transformer 架构内部应用的注意力称为<strong>自注意力（self-attention）</strong>。在自注意力中，每个序列元素提供一个键、值和query。对于每个元素，根据其query作用一个注意力神经层，检查所有序列元素键的相似性，并为每个元素返回一个不同的平均值向量。</p>
<h3 id="自注意力机制">自注意力机制</h3>
<p>自注意力背后的核心概念是缩放点积注意力（Scaled Dot Product Attention）。目标是建立一种注意力机制，序列中的任何元素都可以关注任何其他元素，同时仍能高效计算。</p>
<p>点积注意力将一组查询Q，键K和值V（三者矩阵尺寸为T*d，T为序列长度，d为查询、键或值的维度）。点积注意力的计算方法如下：</p>
<p>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d})V$$</p>
<p><img src="/images/self_attention.png" alt="Scaled Dot-Product Attention Explained | Papers With Code" /></p>
<h3 id="多头注意力机制">多头注意力机制</h3>
<p>缩放点积注意力让模型对一个序列进行“关注”。然而，序列元素通常需要关注多个不同方面，并且单个加权平均值并不是最佳选择。这就是提出多头注意力机制（Multi-Head Attention）的根源，即相同特征上的多个不同的（查询，键，值）三元组。</p>
<p>$$\text{Multihead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O<br />
\ \text{where} \quad \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$</p>
<p><img src="/images/multi_head_attention.png" alt="Multi-Head Attention Explained | Papers With Code" /></p>
<p>多头注意力的一个关键特征是它相对于输入具有置换同变性（permutation-equivariant）。因此，多头注意力实际上不是将输入视为序列，而是视为一组元素。这一特性使得多头注意力模块和 Transformer 架构适用广泛</p>
<h3 id="Transformer编码器">Transformer编码器</h3>
<p>最初，Transformer 模型是为机器翻译而设计的。它是一个编码器-解码器结构，其中编码器将原始语言的句子作为输入并生成基于注意力的表征。而解码器关注编码信息并以自回归方式生成翻译的句子，就像 RNN 一样</p>
<p>编码器由N个相同的模块组成，输入x首先通过上面提到的多头注意力块。使用残差连接将输出添加到原始输入，每一次都有归一化操作</p>
<p><img src="/images/encoder_decoder.png" alt="The Transformer: A Quick Run Through | by Mandar Deshpande | TDS Archive |  Medium" /></p>
<p>残差连接在 Transformer 架构中至关重要。</p>
<p>1、首先，与 ResNet类似，Transformers 层级很深。某些模型的编码器中包含超过 24 个blocks。因此，残差连接对于模型梯度的平滑流动至关重要。</p>
<p>2、如果没有残余连接，原始序列的信息就会丢失。多头注意力层忽略序列中元素的位置，并且只能根据输入特征来学习它。删除残余连接意味着该信息在第一个注意层之后（初始化之后）丢失，并且使用随机初始化的查询和键向量，位置i的输出向量与其原始输入无关。注意力的所有输出都可能表示相似/相同的信息，并且模型没有机会区分哪些信息来自哪个输入元素。</p>
<p>归一化层在 Transformer 架构中也发挥着重要作用，它可以实现更快的训练速度。</p>
<p>除了多头注意力之外，模型中还包括一个小型全连接前馈网络，应用于每一个block。它增加了模型的复杂度，并允许单独对每个序列元素进行转换。</p>
<p><img src="/images/detail_encoder_decoder.png" alt="BERT Model – Bidirectional Encoder Representations from Transformers -  QuantPedia" /></p>
<h2 id="Transformer-大致工作流">Transformer 大致工作流</h2>
<p>**第一步：**获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p>
<p><img src="/images/first_step_of_transformer.jpg" alt="img" /></p>
<p>**第二步：**将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。单词向量矩阵用 $$\mathbf{X}_{n×d} $$表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p>
<p><img src="/images/the_second_step.jpg" alt="img" /></p>
<p><strong>第三步</strong>：将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。</p>
<p><img src="/images/the_third_Step.jpg" alt="img" /></p>
<p>上图 Decoder 接收了 Encoder 的编码矩阵 <strong>C</strong>，然后首先输入一个翻译开始符 “<Begin>”，预测第一个单词 “I”；然后输入翻译开始符 “<Begin>” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p>
<h2 id="Transformer-详细流程">Transformer 详细流程</h2>
<h3 id="单词Embedding和位置Embedding">单词Embedding和位置Embedding</h3>
<p>Transformer 中单词的输入表示 <strong>x</strong>由<strong>单词 Embedding</strong> 和<strong>位置 Embedding</strong> （Positional Encoding）相加得到。单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。**因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。**所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p>
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下</p>
<p>$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_model}) \ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_model})$$</p>
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i ≤ d, 2i+1 ≤ d)。使用这种公式计算 PE 有以下的好处：</p>
<ul>
<li>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li>
<li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</li>
</ul>
<p>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <strong>x</strong>，<strong>x</strong> 就是 Transformer 的输入。</p>
<h3 id="Self-Attention">Self-Attention</h3>
<p><img src="/images/self_attention.jpg" alt="img" /></p>
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 <strong>Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 Residual Connection 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<p><img src="D:%5Cblog%5Csource_posts%5Ctransformers.assets%5Cself_attention.png" alt="Scaled Dot-Product Attention Explained | Papers With Code" /></p>
<p>这是self-attention结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)，在实际中，self-attention接收的是输入（单词的表示向量x组成的矩阵X）或者上一个Encoder block的输出，而QKV正是通过Self-Attention的输出线性变换得到的</p>
<h4 id="QKV的计算">QKV的计算</h4>
<p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵$$W^Q,W^K,W^V$$计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong>，其中$$W^Q,W^K,W^V$$是随机初始的，再通过反向传播来优化的</p>
<p><img src="/images/qkv_calculation.jpg" alt="Frontiers | Research on prediction method of photovoltaic power generation  based on transformer model" /></p>
<p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p>
<p>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d_k})V \quad  d_k是Q,K矩阵的列数，即向量维度$$</p>
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 $$d_k $$的平方根。<strong>Q</strong>乘以<strong>K</strong>的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<strong>Q</strong>乘以 $$K^T$$ ，1234 表示的是句子中的单词。</p>
<p><img src="/images/calculation_of_qk.jpg" alt="img" /></p>
<p>得到$$QK^T $$之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p>
<p><img src="/images/softmax_qk.jpg" alt="img" /></p>
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p>
<p><img src="/images/self_attention_output.jpg" alt="img" /></p>
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 $$Z_1$$ 等于所有单词 i 的值 $$V_i $$根据 attention 系数的比例加在一起得到，如下图所示</p>
<p><img src="/images/z_1_meaning.jpg" alt="img" /></p>
<h3 id="Multi-Head-Attention">Multi-Head Attention</h3>
<p>Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p>
<p><img src="/images/multi_head_attention.jpg" alt="img" /></p>
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p>
<p><img src="/images/8_heads_of_multiple_head_attention.jpg" alt="img" /></p>
<p>得到 8 个输出矩阵 $$Z_1 到 Z_8 $$之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong></p>
<p><img src="/images/calculate_multi_head_output_Z.jpg" alt="img" /></p>
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的</p>
<h3 id="Encoder">Encoder</h3>
<p><img src="/images/encoder_structure.jpg" alt="img" /></p>
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, <strong>Add &amp; Norm, Feed Forward, Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p>
<h4 id="Add-Norm">Add &amp; Norm</h4>
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下</p>
<p>$%%%%%$$$LayerNorm(X+MultiHeadAttention(X)) \ LayerNorm(X+FeedForward(X))$$</p>
<p>其中X表示Multi-Head Attention或者FeedForward的输入，MultiHeadAttention(X)和Feedforward(X)表示输出，输入输出维度是一样的所以可以直接相加，<strong>Add</strong>指X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异部分，在ResNet中常用到<img src="/images/res_net_to_explain_residual.jpg" alt="img" /></p>
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p>
<h4 id="Feed-Forward">Feed Forward</h4>
<p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。$$max(0,XW_1+B_1)W_2+b_2$$    X是输入，Feed Forward最终得到的输出矩阵与维度X一致</p>
<h4 id="组成-Encoder">组成 Encoder</h4>
<p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵$$ X_{(n×d)} ，并输出一个矩阵 O_{(n×d)}$$ 。通过多个 Encoder block 叠加就可以组成 Encoder。</p>
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵（是指加了positional embedding之后的），后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p>
<p><img src="/images/display_final_output_of_encoder.jpg" alt="img" /></p>
<h3 id="Decoder">Decoder</h3>
<p><img src="/images/Decoder_structure.jpg" alt="img" /></p>
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p>
<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li>
<li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</li>
<li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li>
</ul>
<h4 id="第一个Multi-Head-Attention">第一个Multi-Head Attention</h4>
<p>Decoder block的第一个Multi-Head Attention采用了Masked操作，因为在翻译的过程中是顺序翻译的，即翻译完第i个单词，才可以翻译第i+1个单词，通过Masked操作可以防止第i个单词知道i+1个单词之后的信息，下面以“我是一只猫”翻译成“I have a cat”为例</p>
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “<Begin>” 预测出第一个单词为 “I”，然后根据输入 “<Begin> I” 预测下一个单词 “have”。</p>
<p><img src="/images/decoder_predict.jpg" alt="img" /></p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “<Begin> I have a cat <end>”</strong></p>
<p>**第一步：**是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 “<Begin> I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。<img src="/images/input_and_mask_matrix.jpg" alt="img" /></p>
<p><strong>第二步：<strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵</strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和 $$K^T 的乘积 QK^T $$。</p>
<p><img src="/images/Q_times_KT.jpg" alt="img" /></p>
<p><strong>第三步：<strong>在得到 $$QK^T$$ 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用</strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下</p>
<p><img src="/images/the_second_mask.jpg" alt="img" /></p>
<p>得到 $$Mask QK^T 之后在 Mask QK^T$$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p>**第四步：**使用 $$MaskQK^T$$与矩阵 <strong>V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 Z1 是只包含单词 1 信息的。</p>
<p><img src="/images/decoder_get_output_z.jpg" alt="img" /></p>
<p><strong>第五步：<strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $$Z_i$$ ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$$Z_i$$ 然后计算得到第一个 Multi-Head Attention 的输出</strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p>
<h4 id="第二个Multi-Head-Attention">第二个Multi-Head Attention</h4>
<p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C</strong> 计算的。</p>
<p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出 <strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p>
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p>
<h3 id="Softmax-预测输出单词">Softmax 预测输出单词</h3>
<p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p>
<p><img src="/images/softmax_final_output.jpg" alt="img" /></p>
<p>Decoder Softmax 之前的 Z</p>
<p>Softmax 根据输出矩阵的每一行预测下一个单词：</p>
<p><img src="/images/softmax_output_2.jpg" alt="img" /></p>
<p>Decoder Softmax 预测</p>
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p>

            
                
            
        </div>

        

        
        <div class="post-tags">标签：
            
            <a href="/tags/notes/">notes</a>
            
        </div>
        
    </article>
    
        <p style="text-align: center">声明: 本文内容由Jasper创作整理，读者若需转载，请保留出处，谢谢！</p>
    
    
    
</div>

        </div>
    </div>
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            本站已建立&nbsp;<a href="/about" id="siteBuildingTime"></a>&nbsp;天， <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span><br/>
            ©2025-<span id="cpYear"></span> 基于&nbsp;<a href="http://hexo.io" target="_blank" rel="external nofollow noopener" title="Hexo">Hexo</a>
            ，主题采用&nbsp;&nbsp;<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="external nofollow noopener" title="JSimple theme">JSimple</a>
            ，作者&nbsp;<a href="https://github.com/jsp-0615" target="_blank" title="author">Jasper</a>
            ，托管于&nbsp;<a href="https://pages.github.com" target="_blank" rel="external nofollow noopener" title="github">github</a>
            ，<a href="https://jsp-0615.github.io/sitemap.xml" title="网站地图" rel="alternate" type="application/xml">网站地图</a>
        </p>
    </div>
</footer>
</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input"
                   spellcheck="true"
                   type="text"
                   autocomplete="off"
                   placeholder="请输入查询关键词"/>
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script async src="/js/SimpleCore.min.js"></script>
<script defer src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!-- disqus count -->

    


<!-- google analytics -->


<!-- ms clarity -->

</body>
</html>